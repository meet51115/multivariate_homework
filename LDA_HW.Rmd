---
title: "LDA hw"
author: "Meet Bhanushali"
date: "2024-04-24"
output: html_document
---



```{r}

library(MASS)
library(ggplot2)
library(memisc)
library(ROCR)
library(dplyr)
library(klaR)
 
attr <- read.csv("/Users/meet/Desktop/Attrition Data copy.csv")
features <- c("Education", "DistanceFromHome", "EnvironmentSatisfaction", "JobSatisfaction", "Age", "MonthlyIncome", "NumCompaniesWorked", "WorkLifeBalance", "YearsAtCompany")
head(attr)
dim(attr)
str(attr)
attr.data <- as.matrix(attr[,c(6:14)])
row.names(attr.data) <- attr$Sr.no
attr_raw <- cbind(attr.data, as.numeric(as.factor(attr$Attrition)) - 1)
# Map numeric values to categorical labels
#test_raw.df$Attrition <- ifelse(test_raw.df$Attrition == 0, "No", "Yes")
colnames(attr_raw)[9] <- "Attrition"
smp_size_raw <- floor(0.75 * nrow(attr_raw))
train_ind_raw <- sample(nrow(attr_raw), size = smp_size_raw)
train_raw.df <- as.data.frame(attr_raw[train_ind_raw, ])
test_raw.df <- as.data.frame(attr_raw[-train_ind_raw, ])
attr_raw.lda <- lda(formula = train_raw.df$Attrition ~ ., data = train_raw.df)
attr_raw.lda
summary(attr_raw.lda)
print(attr_raw.lda)
plot(attr_raw.lda)
attr_raw.lda.predict <- predict(attr_raw.lda, newdata = test_raw.df)
attr_raw.lda.predict$class
attr_raw.lda.predict$x


# Get the posteriors as a dataframe.
attr_raw.lda.predict.posteriors <- as.data.frame(attr_raw.lda.predict$posterior)
# Map numeric values to categorical labels
test_raw.df$Attrition <- ifelse(test_raw.df$Attrition == 0, "No", "Yes")

pred <- prediction(attr_raw.lda.predict.posteriors[,5], test_raw.df$Attrition)

roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
```
#Inference:
#Lower AUC score indicates model performance is not good.
```{r}
# Iris LDA
data("iris")
iris
head(iris, 3)
str(iris)
r <- lda(formula = Species ~ ., data = iris)
r
summary(r)
print(r)
r$counts
r$means
r$scaling
r$prior
r$lev
r$svd
#singular values (svd) that gives the ratio of the between- and within-group standard deviations on the linear discriminant variables.
r$N
r$call
(prop = r$svd^2/sum(r$svd^2))
#we can use the singular values to compute the amount of the between-group variance that is explained by each linear discriminant. In our example we see that the first linear discriminant explains more than 99% of the between-group variance in the iris dataset.
r2 <- lda(formula = Species ~ ., data = iris, CV = TRUE)
r2
head(r2$class)
#the Maximum a Posteriori Probability (MAP) classification (a factor)
#posterior: posterior probabilities for the classes.
head(r2$posterior, 3)
train <- sample(1:150, 75)
r3 <- lda(Species ~ ., # training model
          iris,
          prior = c(1,1,1)/3,
          subset = train)
plda = predict(object = r3, # predictions
               newdata = iris[-train, ])
head(plda$class)
head(plda$posterior, 6) # posterior prob.
head(plda$x, 3)
plot(r)
plot(r3)
r <- lda(Species ~ .,
         iris,
         prior = c(1,1,1)/3)
prop.lda = r$svd^2/sum(r$svd^2)
plda <- predict(object = r,
                newdata = iris)
dataset = data.frame(species = iris[,"Species"],lda = plda$x)
ggplot(dataset) + geom_point(aes(lda.LD1, lda.LD2, colour = species, shape = species), size = 2.5) + labs(x = paste("LD1 (", percent(prop.lda[1]), ")", sep=""),y = paste("LD2 (", percent(prop.lda[2]), ")", sep=""))

#LDA is likely a good dimensionality reduction technique for this dataset, as it has been able to project the higher-dimensional data (four features: sepal length, sepal width, petal length, and petal width) onto a two-dimensional space (the plane of the scatter plot) while still maintaining some separability between the classes.
#The petal length appears to be a more important feature for classification than the sepal width, since the data points are more separated along the petal length axis.


# lets look at another way to divide a dataset

set.seed(101) # Nothing is random!!
sample_n(iris,10)
# Lets take a sample of 75/25 like before. Dplyr preserves class. 
training_sample <- sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.75,0.25))
train <- iris[training_sample, ]
test <- iris[!training_sample, ]
#lets run LDA like before
lda.iris <- lda(Species ~ ., train)
# do a quick plot to understand how good the model is
plot(lda.iris, col = as.integer(train$Species))
# Sometime bell curves are better
plot(lda.iris, dimen = 1, type = "b")
# THis plot shows the essense of LDA. It puts everything on a line and finds cutoffs. 

#Here are some of the things that can be inferred from the confusion matrix:

#Out of 140 employees, 95 did not leave (no attrition) and 45 left (attrition).
#The model correctly predicted 90 out of the 95 employees who did not leave (no attrition). This is called the true negative rate.
#The model correctly predicted 25 out of the 45 employees who left (attrition). This is called the true positive rate.
#There were 5 false positives, which means that the model predicted that 5 employees would leave (attrition) when they actually did not.
#There were 10 false negatives, which means that the model predicted that 10 employees would not leave (no attrition) when they actually did leave (attrition).


# Partition plots
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="lda")

# Lets focus on accuracy. Table function
lda.train <- predict(lda.iris)
train$lda <- lda.train$class
table(train$lda,train$Species)
# running accuracy on the training set shows how good the model is. It is not an indication of "true" accuracy. We will use the test set to approximate accuracy
lda.test <- predict(lda.iris,test)
test$lda <- lda.test$class
table(test$lda,test$Species)


# Wilk's Lambda and F test for each variablw
m <- manova(cbind(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width)~Species,data=iris)
summary(m,test="Wilks")
summary(m,test="Pillai")
summary.aov(m)
```
#Inference:

#Specifically, the plot shows three partition plots, each representing a different app error rate.  The  petal length and sepal width are on the x and y axes. The color regions correspond to the predicted class label for a given data point. In the top left plot, where the app error rate is 0.2, the data points in blue are classified as belonging to class 1, the green points are classified as class 2, and the yellow points are classified as class 3.

#From the plot, we can see that the model performs better when the petal length and sepal width are more distinct between the classes. In the top left plot (where the error rate is highest), the classes are more overlapping than in the bottom right plot (where the error rate is lowest). This suggests that the model is more confident in its predictions when the data points are clearly separated in terms of petal length and sepal width.


