---
title: "logistic regression hw"
author: "Meet Bhanushali"
date: "2024-04-18"
output: html_document
---


```{r}
library(ggplot2)
library(cowplot)
library(regclass)
library(caret)
library(e1071)
library(pROC)

attrition_data <- read.csv("/Users/meet/Desktop/Attrition Data copy.csv",row.names=1, fill = TRUE)
attrition_data
head(attrition_data)
str(attrition_data)

#Converting necessary columns into Factors

attrition_data$Attrition <- as.factor(attrition_data$Attrition)
attrition_data$Education <- as.factor(attrition_data$Education)
attrition_data$JobSatisfaction <- as.factor(attrition_data$JobSatisfaction)
attrition_data$EnvironmentSatisfaction <- as.factor(attrition_data$EnvironmentSatisfaction)
attrition_data$WorkLifeBalance <- as.factor(attrition_data$WorkLifeBalance)

nrow(attrition_data)

## Exploratory Analysis

xtabs(~ Attrition + Education, data=attrition_data)
xtabs(~ Attrition + JobSatisfaction, data=attrition_data)
xtabs(~ Attrition + EnvironmentSatisfaction, data=attrition_data)
xtabs(~ Attrition + WorkLifeBalance, data=attrition_data)

#Model Development

logistic_simple <- glm(Attrition ~ Education+JobSatisfaction+EnvironmentSatisfaction+WorkLifeBalance, data=attrition_data, family="binomial")
summary(logistic_simple)

#Most of the predictor variables ("Education", "JobSatisfaction", etc.) have coefficients with statistically significant p-values (all p-values are greater than alpha). This suggests that there is insufficient evidence to conclude that any of these variables have a significant impact on Attrition.

#For Job Satisfaction = 1, odds of attrition are:
job1.log.odds <- log(66 / 223)
job1.log.odds

#For Job Satisfaction = 2, odds of attrition are:
job2.log.odds <- log(46 / 234)
job2.log.odds

#For Job Satisfaction = 3, odds of attrition are:
job3.log.odds <- log(73 / 369)
job3.log.odds

#For Job Satisfaction = 4, odds of attrition are:
job4.log.odds <- log(52 / 487)
job4.log.odds

predicted.attrition_data <- data.frame(probability.of.Attrition=logistic_simple$fitted.values,JobSatisfaction=attrition_data$JobSatisfaction)
predicted.attrition_data


xtabs(~ probability.of.Attrition + JobSatisfaction, data=predicted.attrition_data)
logistic <- glm(Attrition ~ ., data=attrition_data, family="binomial")
summary(logistic)


## Now calculate the overall "Pseudo R-squared" and its p-value
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
(ll.null - ll.proposed) / ll.null
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
predicted.attrition_data <- data.frame(probability.of.Attrition=logistic$fitted.values,Attrition=attrition_data$Attrition)
predicted.attrition_data <- predicted.attrition_data[order(predicted.attrition_data$probability.of.Attrition, decreasing=FALSE),]
predicted.attrition_data$rank <- 1:nrow(predicted.attrition_data)

# The Pseudo R-squared measures the proportion of total variance in the response variable that is explained by the model.
# The output of 0.2 indicates that the model does not fits the data perfectly.

# As the p-value for R^2 is less than 0.05, we reject the null hypothesis.

plot(logistic)

# Residuals vs Fitted are a great way to identify heteroscedasticity via pattern recognition, however our model does not seem to have any.
# Q-Q Plots are great in understanding the distribution of residuals. As it can be observed that there is quite some deviation from the straight line indicating that residuals do not follow a normal distribution.
# Scale-Location Plot shows the square root of the standardized residuals against the fitted values. As it may be observed that the spread of points is not consistent across the range of fitted values, it indicates that the variability of the residuals do not remain constant.
# Residuals Vs Leverage help in identifying potential problems with the regression model. Points on the right of the plot have a great influence on the model's parameters. Points with a Cook's Distance greater than highlighted threshold have a greater influence on the model.

ggplot(data=predicted.attrition_data, aes(x=rank, y=probability.of.Attrition)) +
geom_point(aes(color=Attrition), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of Attrition")

# This plot highlights the performace of our model.
# It can be clearly inferred that the model performs poorly.

# From Caret
pdata <- predict(logistic,newdata=attrition_data,type="response" )
pdata
attrition_data$Attrition
pdataN <- as.factor(ifelse(test=as.numeric(pdata>0.5) == 0, yes="No", no="Yes"))

#Each value in the pdata object represents the predicted probability of Attrition being "Yes" for the corresponding observation in your dataset.

#From e1071::
confusionMatrix(pdataN, attrition_data$Attrition)
# From pROC
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE)
par(pty = "s")
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE)


roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE)
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage")

roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4)
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4)
## If we want to find out the optimal threshold we can store the
## data used to make the ROC graph in a variable...
roc.info <- roc(attrition_data$Attrition, logistic$fitted.values, legacy.axes=TRUE)
str(roc.info)
## tpp = true positive percentage
## fpp = false positive precentage
roc.df <- data.frame(tpp=roc.info$sensitivities*100, fpp=(1 - roc.info$specificities)*100,thresholds=roc.info$thresholds)
roc.df
head(roc.df) 
## head() will show us the values for the upper right-hand corner of the ROC graph, when the threshold is so low
## Thus TPP = 100% and FPP = 100%
tail(roc.df) 

## tail() will show us the values for the lower left-hand corner
## of the ROC graph, when the threshold is so high (infinity)
## that every single sample is called "not obese".
## Thus, TPP = 0% and FPP = 0%
## now let's look at the thresholds between TPP 60% and 80%
roc.df[roc.df$tpp > 60 & roc.df$tpp < 80,]
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE)
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE)
roc(attrition_data$Attrition,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE, partial.auc=c(100, 90), auc.polygon = TRUE, auc.polygon.col = "#377eb822", print.auc.x=45)
# Lets do two roc plots to understand which model is better
roc(attrition_data$Attrition, logistic_simple$fitted.values, plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, print.auc=TRUE)
# Lets add the other graph
plot.roc(attrition_data$Attrition, logistic$fitted.values, percent=TRUE, col="#4daf4a", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=40)
legend("bottomright", legend=c("Simple", "Non Simple"), col=c("#377eb8", "#4daf4a"), lwd=4) 

```
**Inference:**
*Not Attrited* - Since we are aiming to see how many have not Attrition, based on their background factors.
* The accuracy of the model is very less showcasing that it is not a well performing model. However, such low accuracy is a clear indication of model underfitting.
* At 95% Confidence interval, model is not significant.

#### **SUMMARY**

* We can infer that due to the vary dataset, it was difficult to fit the model well which led to an under-performing model.
* Another aspect that calls to attention the bias present in the data is how majority of our logged data points had a 'No' for Attrition with just few "yes" for the data points.

